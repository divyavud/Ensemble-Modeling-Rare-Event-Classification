{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rare Event Classification (Ensemble Modelling incorporating Random Undersampling)\n",
    "\n",
    "\n",
    "**About the Dataset:**\n",
    "\n",
    "The data consists of 10,500 credit applications, each classified as good or bad credit. However, there are only 500 bad credit applications. Since this is less than 5% of the data, classifying applicants as bad credit is referred to as a rare event problem. This is also known as anomaly dete ction in many applications.\n",
    "\n",
    "\n",
    "**Approach:**\n",
    "\n",
    "1. The best ratio is discovered by trying ratios between 50:50 to 85:15. \n",
    "2. Build an ensemble model based on the optimum ratio selected. \n",
    "\n",
    "This is done my creating ensemble of trees using the optimum ratio, fitting a model to each, making classification probability predictions for each and then averaging those to get predicted classification probabilities. From that we can calculate the loss totaled over all the trees.\n",
    "\n",
    "The base model is a decision tree with a minimum leaf size is 5, and the minimum split size is 5. The optimum depth for this model is determined by optimizing the F1-score using 10-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Required Libraries\n",
    "\n",
    "# Install using Conda:\n",
    "# conda install -c glemaitre imbalanced-learn\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from AdvancedAnalytics import ReplaceImputeEncode\n",
    "# classes for decision tree\n",
    "from AdvancedAnalytics import DecisionTree, calculate\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import export_graphviz\n",
    "from pydotplus.graphviz import graph_from_dot_data\n",
    "import graphviz\n",
    "import math\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>good_bad</th>\n",
       "      <th>age</th>\n",
       "      <th>amount</th>\n",
       "      <th>duration</th>\n",
       "      <th>checking</th>\n",
       "      <th>coapp</th>\n",
       "      <th>depends</th>\n",
       "      <th>employed</th>\n",
       "      <th>existcr</th>\n",
       "      <th>foreign</th>\n",
       "      <th>history</th>\n",
       "      <th>housing</th>\n",
       "      <th>installp</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>other</th>\n",
       "      <th>property</th>\n",
       "      <th>resident</th>\n",
       "      <th>savings</th>\n",
       "      <th>telephon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>good</td>\n",
       "      <td>67</td>\n",
       "      <td>1169</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>good</td>\n",
       "      <td>67</td>\n",
       "      <td>1169</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>good</td>\n",
       "      <td>67</td>\n",
       "      <td>1169</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>good</td>\n",
       "      <td>67</td>\n",
       "      <td>1169</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>good</td>\n",
       "      <td>67</td>\n",
       "      <td>1169</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  good_bad  age  amount  duration  checking  coapp  depends  employed  \\\n",
       "0     good   67    1169         6         1      1        1         5   \n",
       "1     good   67    1169         6         1      1        1         5   \n",
       "2     good   67    1169         6         1      1        1         5   \n",
       "3     good   67    1169         6         1      1        1         5   \n",
       "4     good   67    1169         6         1      1        1         5   \n",
       "\n",
       "   existcr  foreign  history  housing  installp  job  marital  other  \\\n",
       "0        2        1        4        2         4    3        3      3   \n",
       "1        2        1        4        2         4    3        3      3   \n",
       "2        2        1        4        2         4    3        3      3   \n",
       "3        2        1        4        2         4    3        3      3   \n",
       "4        2        1        4        2         4    3        3      3   \n",
       "\n",
       "   property  resident  savings  telephon  \n",
       "0         1         4        5         2  \n",
       "1         1         4        5         2  \n",
       "2         1         4        5         2  \n",
       "3         1         4        5         2  \n",
       "4         1         4        5         2  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reading Data\n",
    "df = pd.read_excel(\"CreditData_RareEvent.xlsx\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "good    10000\n",
       "bad       500\n",
       "Name: good_bad, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Minority classes are 500\n",
    "df['good_bad'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss Calculation Function**\n",
    "\n",
    "The following function can be used to calculate loss and the confusion matrix for our models. This is useful since the loss calculations are a function of the Amount of the loan application. If the case is correctly classified by the model, the loss is zero. Otherwise the loss is a function of the loan amount, which is different for false positives and negatives.\n",
    "\n",
    "Loss = Amount, if the case is a false positive, or\n",
    "Loss = 0.15 x Amount, if the case is a false negative.\n",
    "\n",
    "False positives are loans that were classified as good but the customer later defaults on the loan. In that case, the entire amount of the loan is treated as the loss. In practice, this amount might be adjusted by the actual loss for the load which would be the loan amount minus payments, plus some overhead costs. Here, we are just using the unadjusted loan amount.\n",
    "\n",
    "False negatives are applications that were classified as bad but should have been classified as good. That is the customer would have paid off the loan in a timely fashion, but the model is saying they should be denied a loan.\n",
    "\n",
    "In this function, the numpy array y is the actual classification for each case. It is encoded using zeros for the bad classifications and one for the good classifications because alphabetically bad occurs before good. If instead of bad and good, the data used yes and no, respectively, then the bad classifications would have been coded as ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculating loss and confusion matrix\n",
    "def loss_cal(y, y_predict, fp_cost, fn_cost, display=True):\n",
    "    loss = [0, 0] #False Neg Cost, False Pos Cost\n",
    "    conf_mat = [0, 0, 0, 0] #tn, fp, fn, tp\n",
    "    for j in range(len(y)):\n",
    "        if y[j]==0:\n",
    "            if y_predict[j]==0:\n",
    "                conf_mat[0] += 1 #True Negative\n",
    "            else:\n",
    "                conf_mat[1] += 1 #False Positive\n",
    "                loss[1] += fp_cost[j]\n",
    "        else:\n",
    "            if y_predict[j]==1:\n",
    "                conf_mat[3] += 1 #True Positive\n",
    "            else:\n",
    "                conf_mat[2] += 1 #False Negative\n",
    "                loss[0] += fn_cost[j]\n",
    "    if display:\n",
    "        fn_loss = loss[0]\n",
    "        fp_loss = loss[1]\n",
    "        total_loss = fn_loss + fp_loss\n",
    "        misc = conf_mat[1] + conf_mat[2]\n",
    "        misc = misc/len(y)\n",
    "        print(\"{:.<23s}{:10.4f}\".format(\"Misclassification Rate\", misc))\n",
    "        print(\"{:.<23s}{:10.0f}\".format(\"False Negative Cost\", fn_loss))\n",
    "        print(\"{:.<23s}{:10.0f}\".format(\"False Positive Cost\", fp_loss))\n",
    "        print(\"{:.<23s}{:10.0f}\".format(\"Total Loss\", total_loss))\n",
    "    return loss, conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attribute Map for CreditData_RareEvent.xlsx, N=10,500\n",
    "attribute_map = { \\\n",
    "'age':['I',(19,120)], \\\n",
    "'amount': ['I',(0,20000)], \\\n",
    "'checking': ['N',(1,2,3,4)], \\\n",
    "'coapp': ['N',(1,2,3)], \\\n",
    "'depends': ['B',(1,2)], \\\n",
    "'duration': ['I',(1,72)], \\\n",
    "'employed': ['N',(1,2,3,4,5)], \\\n",
    "'existcr': ['N',(1,2,3,4)], \\\n",
    "'foreign': ['B',(1,2)], \\\n",
    "'good_bad': ['B',('bad','good')], \\\n",
    "'history': ['N',(0,1,2,3,4)], \\\n",
    "'housing':['N',(1,2,3)], \\\n",
    "'installp': ['N',(1,2,3,4)], \\\n",
    "'job': ['N',(1,2,3,4)], \\\n",
    "'marital': ['N',(1,2,3,4)], \\\n",
    "'other': ['N',(1,2,3)], \\\n",
    "'property': ['N',(1,2,3,4)], \\\n",
    "'resident': ['N',(1,2,3,4)], \\\n",
    "'savings': ['N',(1,2,3,4,5)], \\\n",
    "'telephon': ['B',(1,2)] \\\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encode the categorical attributes using one-hot encoding. Since this is a decision tree model, the last one-hot column is not dropped. The interval attributes are scaled using z-score scaling. This is not required, but can improve the speed of the fitting the decision trees.\n",
    "\n",
    "In these data, the target attribute good_bad is entered as good and bad rather than one and zero. The ReplaceImputeEncode method will encode the character sting version of good_bad into zeros and ones, but zero is used to encode bad and one is used to encode good. As a result, a false positive refers to classifying the target as 1, or equivalently as good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Preprocessing\n",
    "\n",
    "# Encode for Logistic Regression, drop last one-hot column\n",
    "rie = ReplaceImputeEncode(data_map=attribute_map, nominal_encoding='one-hot', interval_scale = 'std', \\\n",
    "                          drop=False, display=False)\n",
    "encoded_df = rie.fit_transform(df)\n",
    "# Create X and y, numpy arrays\n",
    "# The target is not scaled or imputed, but\n",
    "# the target coding is: bad=0 and good=1\n",
    "X = np.asarray(encoded_df.drop('good_bad',axis=1))\n",
    "y = np.asarray(encoded_df['good_bad'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate Potential Loss for Each Case**\n",
    "\n",
    "The potential loss is the false positive and false negative loss calculated assuming the case might be classifed as false positive or negative. In most cases the model will correctly classify the observation and the actual loss will be zero. The potential loss is used to evaluate models as they are developed. In this example the potential losses are calculated using the following formulas, one for false positive and another for false negatives:\n",
    "\n",
    "- False Positive Cost = Amount, and False Negative = 0.15 x Amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model is one that minimizes the loss\n",
    "# Setup false positive and false negative costs for each transaction\n",
    "fp_cost = np.array(df['amount'])\n",
    "fn_cost = np.array(0.1*df['amount'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate Total Loss without RUS**\n",
    "\n",
    "As a benchmark, it is good to know the results from fitting the entire dataset without using RUS. The following code evaluates fitting the entire dataset using a Decision Tree built using 10-fold cross validation to determine the optimum depth. In this case depths between 2 and 20 are examined. The depth that maximizes the F1-score is selected as the optimum depth.\n",
    "\n",
    "In this cross validation, the optimum depth is 2 with F1 = 97.6%. The misclassification rate if 4.7%, which is almost equal to the percent of bad credit cases in these data. The accuracy is high, 95%. However, notice that the bad credit applicants are being ignored. Most of the 500 applicants with bad credit are being classified as good. The model is classifying all but 5 applicants as good credit risks. From the perspective of the quality metrics this is an excellent model, but from the perspective of a banker who needs to reduce loss from bad loans, this is a terrible model. The estimated total loss from this model is $2,086,650, and it is all from classifying applicants with bad credit as good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decision Tree constructed using Depth =  2 and all data.\n",
      "Misclassification Rate.    0.0471\n",
      "False Negative Loss....         0\n",
      "False Positive Loss....   2086650\n",
      "Total Loss.............   2086650\n",
      "\n",
      "Model Metrics\n",
      "Observations...............     10500\n",
      "Features...................        58\n",
      "Maximum Tree Depth.........         2\n",
      "Minimum Leaf Size..........         5\n",
      "Minimum split Size.........         5\n",
      "Mean Absolute Error........    0.0860\n",
      "Avg Squared Error..........    0.0430\n",
      "Accuracy...................    0.9529\n",
      "Precision..................    0.9528\n",
      "Recall (Sensitivity).......    1.0000\n",
      "F1-Score...................    0.9758\n",
      "MISC (Misclassification)...      4.7%\n",
      "     class 0...............     99.0%\n",
      "     class 1...............      0.0%\n",
      "\n",
      "\n",
      "     Confusion\n",
      "       Matrix     Class 0   Class 1  \n",
      "Class 0.....         5       495\n",
      "Class 1.....         0     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "search_depths = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
    "best_d = 0\n",
    "max_f = 0\n",
    "for d in search_depths:\n",
    "    dtc = DecisionTreeClassifier(max_depth=d, min_samples_leaf=5, \\\n",
    "    min_samples_split=5, criterion='gini')\n",
    "    dtc_10 = cross_val_score(dtc, X, y, scoring='f1', cv=10)\n",
    "    mean = dtc_10.mean()\n",
    "    if mean > max_f:\n",
    "        max_f = mean\n",
    "        best_d = d\n",
    "        best_dtc = dtc\n",
    "        print(\"\\nDecision Tree constructed using Depth = \",best_d, \"and all data.\")\n",
    "best_dtc.fit(X, y)\n",
    "loss,conf_mat = calculate.binary_loss(y,best_dtc.predict(X),\\\n",
    "fp_cost,fn_cost)\n",
    "DecisionTree.display_binary_metrics(best_dtc, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cross validation, the optimum depth is 2 with F1 = 97.6%. The misclassification rate if 4.7%, which is almost equal to the percent of bad credit cases in these data. The accuracy is high, 95%. However, notice that the bad credit applicants are being ignored. Most of the 500 applicants with bad credit are being classified as good. The model is classifying all but 5 applicants as good credit risks.\n",
    "\n",
    "From the perspective of the quality metrics this is an excellent model, but from the perspective of a banker who needs to reduce loss from bad loans, this is a terrible model. The estimated total loss from this model is $2,086,650, and it is all from classifying applicants with bad credit as good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Undersampling Technique**\n",
    "\n",
    "Building a model that decreases this loss involves using RUS. The first step is to identify the best mixture of majority and minority event applicants. This example considers mixtures of 50:50, 60:40, 70:30, 75:25, 80:20 and 85:15.\n",
    "\n",
    "In Python, instead of passing these ratios into the RUS routine, it is necessary to pass the actual number of observations represented by these mixtures. For example, the 50:50 ratio is a sample constructed using all 500 bad applicants and a random sample of an additional 500 good applications. The total sample is 1,000 applications evenly divided between bad and good.\n",
    "\n",
    "A mixture of 80:20 would have 80% good and 20% bad. Since each RUS sample is constructed using all of the minority data, all 500 of the bad applicants, the number of randomly selected good applicants will need to be 4 times larger. That is, the number of randomly selected good applications to achieve an 80:20 ratio of good to bad cases is calculated by: (0.8/0.2) x 500 = 2,000.\n",
    "\n",
    "After making these calculations for each ratio, we create list containing the random seeds we would like to use, rand_val, a list of ratios, ratio, and a tuple, rus_ratio, containing dictionaries that describe the number of observations for each ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup 10 random number seeds for use in creating random samples\n",
    "np.random.seed(12345)\n",
    "max_seed = 2**20-1\n",
    "rand_val = np.random.randint(1, high=max_seed, size=10)\n",
    "\n",
    "# Use majority:minority ratios of 50:50, 60:40, 70:30, 75:25, 80:20, 85:15\n",
    "ratio = [ '50:50', '60:40', '70:30', '75:25', '80:20', '85:15' ]\n",
    "\n",
    "# Dictionaries contains number of minority and majority\n",
    "# n_majority = ratio x n_minority\n",
    "rus_ratio = ({0:500, 1:500}, {0:500, 1:750}, {0:500, 1:1167}, {0:500, 1:1500}, {0:500, 1:2000}, {0:500, 1:2833})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decision Tree Classifier Model using 50:50 RUS\n",
      "Best depth.............    1.50E+01\n",
      "Misclassification Rate.      0.2097\n",
      "False Negative Loss.... $   741,810\n",
      "False Positive Loss.... $   121,370\n",
      "Total Loss............. $   863,179 +/- $35,351\n",
      "\n",
      "Decision Tree Classifier Model using 60:40 RUS\n",
      "Best depth.............    1.80E+01\n",
      "Misclassification Rate.      0.1567\n",
      "False Negative Loss.... $   544,674\n",
      "False Positive Loss.... $   153,779\n",
      "Total Loss............. $   698,453 +/- $19,577\n",
      "\n",
      "Decision Tree Classifier Model using 70:30 RUS\n",
      "Best depth.............    1.80E+01\n",
      "Misclassification Rate.      0.0970\n",
      "False Negative Loss.... $   326,478\n",
      "False Positive Loss.... $   182,219\n",
      "Total Loss............. $   508,697 +/- $13,300\n",
      "\n",
      "Decision Tree Classifier Model using 75:25 RUS\n",
      "Best depth.............    2.00E+01\n",
      "Misclassification Rate.      0.0790\n",
      "False Negative Loss.... $   267,626\n",
      "False Positive Loss.... $   179,723\n",
      "Total Loss............. $   447,349 +/- $10,244\n",
      "\n",
      "Decision Tree Classifier Model using 80:20 RUS\n",
      "Best depth.............    2.00E+01\n",
      "Misclassification Rate.      0.0549\n",
      "False Negative Loss.... $   174,318\n",
      "False Positive Loss.... $   211,310\n",
      "Total Loss............. $   385,628 +/- $9,998\n",
      "\n",
      "Decision Tree Classifier Model using 85:15 RUS\n",
      "Best depth.............    2.00E+01\n",
      "Misclassification Rate.      0.0388\n",
      "False Negative Loss.... $   122,790\n",
      "False Positive Loss.... $   245,355\n",
      "Total Loss............. $   368,145 +/- $14,491\n",
      "\n",
      "Best RUS Ratio.........       85:15\n",
      "Best C.................    2.00E+01\n",
      "Lowest Loss............ $   368,145 +/- $14,491\n"
     ]
    }
   ],
   "source": [
    "# Use a decision tree as the base model. \n",
    "# Build upon the ‘gini’ split criterion and optimize the depth for values between 2 and 20.\n",
    "\n",
    "depth_list = list(range(2,21))\n",
    "\n",
    "min_loss = 1e64\n",
    "best_ratio = 0\n",
    "for k in range(len(rus_ratio)):\n",
    "    print(\"\\nDecision Tree Classifier Model using \" + ratio[k] + \" RUS\")\n",
    "    best_d = 0\n",
    "    min_loss_c = 1e64\n",
    "    for j in range(len(depth_list)):\n",
    "        d = depth_list[j]\n",
    "        fn_loss = np.zeros(len(rand_val))\n",
    "        fp_loss = np.zeros(len(rand_val))\n",
    "        misc = np.zeros(len(rand_val))\n",
    "        for i in range(len(rand_val)):\n",
    "            rus = RandomUnderSampler(ratio=rus_ratio[k], random_state=rand_val[i], \\\n",
    "                                     return_indices=False, replacement=False)\n",
    "            X_rus, y_rus = rus.fit_sample(X, y)\n",
    "            \n",
    "            dtc = DecisionTreeClassifier(criterion='gini', max_depth=d, min_samples_leaf=5, min_samples_split=5)\n",
    "            dtc = dtc.fit(X_rus, y_rus)\n",
    "            \n",
    "            loss, conf_mat = calculate.binary_loss(y, dtc.predict(X), fp_cost, fn_cost, display=False)\n",
    "            \n",
    "            fn_loss[i] = loss[0]\n",
    "            fp_loss[i] = loss[1]\n",
    "            misc[i] = (conf_mat[1] + conf_mat[2])/y.shape[0]\n",
    "        avg_misc = np.average(misc)\n",
    "        t_loss = fp_loss+fn_loss\n",
    "        avg_loss = np.average(t_loss)\n",
    "        if avg_loss < min_loss_c:\n",
    "            min_loss_c = avg_loss\n",
    "            se_loss_c = np.std(t_loss)/math.sqrt(len(rand_val))\n",
    "            best_d = d\n",
    "            misc_c = avg_misc\n",
    "            fn_avg_loss = np.average(fn_loss)\n",
    "            fp_avg_loss = np.average(fp_loss)\n",
    "    if min_loss_c < min_loss:\n",
    "        min_loss = min_loss_c\n",
    "        se_loss = se_loss_c\n",
    "        best_ratio = k\n",
    "        best_reg = best_d\n",
    "    print(\"{:.<23s}{:12.2E}\".format(\"Best depth\", best_d))\n",
    "    print(\"{:.<23s}{:12.4f}\".format(\"Misclassification Rate\",misc_c))\n",
    "    print(\"{:.<23s} ${:10,.0f}\".format(\"False Negative Loss\",fn_avg_loss))\n",
    "    print(\"{:.<23s} ${:10,.0f}\".format(\"False Positive Loss\",fp_avg_loss))\n",
    "    print(\"{:.<23s} ${:10,.0f}{:5s}${:<,.0f}\".format(\"Total Loss\", min_loss_c, \" +/- \", se_loss_c))\n",
    "print(\"\")\n",
    "print(\"{:.<23s}{:>12s}\".format(\"Best RUS Ratio\", ratio[best_ratio]))\n",
    "print(\"{:.<23s}{:12.2E}\".format(\"Best C\", best_reg))\n",
    "print(\"{:.<23s} ${:10,.0f}{:5s}${:<,.0f}\".format(\"Lowest Loss\", \\\n",
    "min_loss, \" +/-\", se_loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this search, the best ratio is defined as the ratio with the lowest calculated loss, after optimizing the tree depth. The best ratio is 80:15 with a depth of 20 levels. With that configuration, the loss calculated over the entire dataset is estimated to be $368,145.\n",
    "\n",
    "This is significantly lower than the loss calculated for an optimized tree fitted to the entire dataset. In that case the tree classified all by 5 cases as good credit. The quality metrics were high, but the calculated loss was a little over $2 million. The loss using the base model is over 4 times the loss projected using RUS.\n",
    "\n",
    "Also of interest is the misclassification error. The error for the base model was 4.7%. The same error for the RUS model is projected to be 3.6%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ensemble Modelling-RUS** \n",
    "\n",
    "From the first step, it is clear that the best ratio identified is 85:15 with a tree with 19 levels. The final step in RUS modeling is to build an ensemble model using the best ratio, and to estimate of the total loss from the ensemble model.\n",
    "\n",
    "An ensemble model, in this case, is the average of several models, each developed using the same best ratio, but with different random samples. Each 85:15 RUS sample uses all 500 cases from the bad applicants and then an additional 4,500 cases randomly selected from the remaining 10,000 good applicants. Since these are randomly selected, each sample produces different estimates of the Decision Tree.\n",
    "\n",
    "In this example, 100 separate samples are used to create 100 trees. Each produces different estimates of the probability that the applicant is a bad credit risk. The ensemble model averages the 100 estimates for each of the 10,500 cases in the data. These average probabilities are used to classify the data and finally to evaluate the misclassification rate and total loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ensemble Estimates based on averaging 100 Models\n",
      "Misclassification Rate.    0.0020\n",
      "False Negative Loss....         0\n",
      "False Positive Loss....     63181\n",
      "Total Loss.............     63181\n"
     ]
    }
   ],
   "source": [
    "# Ensemble Modeling - Averaging Classification Probabilities\n",
    "n_obs = len(y)\n",
    "n_rand = 100\n",
    "predicted_prob = np.zeros((n_obs,n_rand))\n",
    "avg_prob = np.zeros(n_obs)\n",
    "# Setup 100 random number seeds for use in creating random samples\n",
    "np.random.seed(12345)\n",
    "max_seed = 2**20-1\n",
    "rand_value = np.random.randint(1, high=max_seed, size=n_rand)\n",
    "# Model 100 random samples, each with a 70:30 ratio\n",
    "for i in range(len(rand_value)):\n",
    "    rus = RandomUnderSampler(ratio=rus_ratio[best_ratio], \\\n",
    "                             random_state=rand_value[i], return_indices=False, \\\n",
    "                             replacement=False)\n",
    "    X_rus, y_rus = rus.fit_sample(X, y)\n",
    "    \n",
    "    dtc = DecisionTreeClassifier(criterion='gini', max_depth=d, min_samples_leaf=5, min_samples_split=5)\n",
    "    dtc = dtc.fit(X_rus, y_rus)\n",
    "    \n",
    "    predicted_prob[0:n_obs, i] = dtc.predict_proba(X)[0:n_obs, 0]\n",
    "for i in range(n_obs):\n",
    "    avg_prob[i] = np.mean(predicted_prob[i,0:n_rand])\n",
    "# Set y_pred equal to the predicted classification\n",
    "y_pred = avg_prob[0:n_obs] < 0.5\n",
    "y_pred.astype(np.int)\n",
    "# Calculate loss from using the ensemble predictions\n",
    "print(\"\\nEnsemble Estimates based on averaging\",len(rand_value), \"Models\")\n",
    "loss, conf_mat = calculate.binary_loss(y, y_pred, fp_cost, fn_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ensemble model is significantly better than the best RUS model. The misclassification error for the ensemble model is only 0.2%. The rate for the base model was 4.7% and the RUS model 3.6%.\n",
    "\n",
    "The estimated loss for the ensemble model is $63,181\n",
    "\n",
    "The estimated loss for the base model was over 2 million"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
